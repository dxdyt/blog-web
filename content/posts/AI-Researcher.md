---
title: AI-Researcher
date: 2025-09-22T12:22:56+08:00
draft: False
featuredImage: https://images.unsplash.com/photo-1755918546836-fb909c73e999?ixid=M3w0NjAwMjJ8MHwxfHJhbmRvbXx8fHx8fHx8fDE3NTg1MTQ4Nzl8&ixlib=rb-4.1.0
featuredImagePreview: https://images.unsplash.com/photo-1755918546836-fb909c73e999?ixid=M3w0NjAwMjJ8MHwxfHJhbmRvbXx8fHx8fHx8fDE3NTg1MTQ4Nzl8&ixlib=rb-4.1.0
---

# [HKUDS/AI-Researcher](https://github.com/HKUDS/AI-Researcher)

<a name="readme-top"></a>

<div align="center">
  <img src="./assets/ai-researcher.png" alt="Logo" width="400">
  <h1 align="center">"AI-Researcher: Autonomous Scientific Innovation"
 </h1>
</div>

<div align="center">
<a href="https://trendshift.io/repositories/14638" target="_blank"><img src="https://trendshift.io/api/badge/repositories/14638" alt="HKUDS%2FAI-Researcher | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>
</div>

<div align="center">
  <a href="https://autoresearcher.github.io"><img src="https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&color=FFE165&logo=homepage&logoColor=white" alt="Project Page"></a>
  <a href="https://join.slack.com/t/ai-researchergroup/shared_invite/zt-30y5a070k-C0ajQt1zmVczFnfGkIicvA"><img src="https://img.shields.io/badge/Slack-Join%20Us-red?logo=slack&logoColor=white&style=for-the-badge" alt="Join our Slack community"></a>
  <a href="https://discord.gg/zBNYTk5q2g"><img src="https://img.shields.io/badge/Discord-Join%20Us-purple?logo=discord&logoColor=white&style=for-the-badge" alt="Join our Discord community"></a>
  <br/>
  <a href="https://autoresearcher.github.io/docs"><img src="https://img.shields.io/badge/Documentation-000?logo=googledocs&logoColor=FFE165&style=for-the-badge" alt="Check out the documentation"></a>
  <a href="https://arxiv.org/abs/2505.18705"><img src="https://img.shields.io/badge/Paper%20on%20Arxiv-000?logoColor=FFE165&logo=arxiv&style=for-the-badge" alt="Paper"></a>
  <a href="https://autoresearcher.github.io/leaderboard"><img src="https://img.shields.io/badge/DATASETS-000?logoColor=FFE165&logo=huggingface&style=for-the-badge" alt="Benchmark"></a>
  <hr>
</div>

Welcome to **AI-Researcher**ü§ó AI-Researcher introduces a revolutionary breakthrough in **Automated Scientific Discovery**üî¨, presenting a new system that fundamentally **Reshapes the Traditional Research Paradigm**. This state-of-the-art platform empowers researchers with:

 - üéØ **Full Autonomy**: Complete end-to-end research automation
 - üîÑ **Seamless Orchestration**: From concept to publication
 - üß† **Advanced AI Integration**: Powered by cutting-edge AI agents
 - üöÄ **Research Acceleration**: Streamlined scientific innovation

--------------------------------------------------------------------------------

‚ú® The AI-Researcher system accepts user input queries at two distinct levels ‚ú®

**Level 1: Detailed Idea Description**
<br/> At this level, users provide comprehensive descriptions of their specific research ideas. The system processes these detailed inputs to develop implementation strategies based on the user's explicit requirements.

**Level 2: Reference-Based Ideation**
<br/> This simpler level involves users submitting reference papers without a specific idea in mind. The user query typically follows the format: "I have some reference papers, please come up with an innovative idea and implement it with these papers." The system then analyzes the provided references to generate and develop novel research concepts.

--------------------------------------------------------------------------------

üåü**Core Capabilities & Integration**</br>
**AI-Researcher** delivers a **Comprehensive Research Ecosystem** through seamless integration of critical components:

üöÄ**Primary Research Functions**
 - üìö **Literature Review**: Conducts comprehensive analysis and synthesis of existing research.
 - üìä **Idea Generation**: Systematically gathers, organizes, and formulates novel research directions.
 - üß™ **Algorithm Design and Implementation**: Develops methodologies and transforms ideas into functional implementations.
 - üíª **Algorithm Validation and Refinement**: Automates testing, performance evaluation, and iterative optimization.
 - üìà **Result Analysis**: Delivers advanced interpretation of experimental data and insights.
 - ‚úçÔ∏è **Manuscript Creation**: Automatically generates polished, full-length academic papers.

<div align="center">
  <!-- <img src="./assets/AI-Researchernew-intro.pdf" alt="Logo" width="100%"> -->
  <figure>
    <img src="./assets/AI-Researcher-Framework.png" alt="Logo" style="max-width: 100%; height: auto;">
    <br>
    <figcaption><em>Quick Overview of AI-Researcher.</em></figcaption>
  </figure>
</div>


<span id='news'/>

## üî• News

<div class="scrollable">
    <ul>
      <li><strong>[2025. 09]</strong>: &nbsp; üéØüéØüì¢üì¢ Exciting News! We are thrilled to announce that our üåüAI-Researcherüåü has been accepted as a Spotlight paper at NeurIPS 2025! üéâüéâ Thanks to all the team members ü§ó </b>
      </li>
      <li><strong>[2025. 05]</strong>: &nbsp;üéâüéâ <b>Major Release! AI-Researcher Comprehensive Upgrade!</b> üöÄ
        <br>We are excited to announce a significant milestone for AI-Researcher:
        <ul>
          <li>üìÑ <b><a href="https://arxiv.org/abs/2505.18705">Academic Paper Release</a></b>: Detailed exposition of our innovative methods and experimental results</li>
          <li>üìä <b><a href="https://autoresearcher.github.io/leaderboard">Benchmark Suite</a></b>: Comprehensive evaluation framework and datasets</li>
          <li>üñ•Ô∏è <b>Web GUI Interface</b>: User-friendly graphical interface making research more convenient</li>
        </ul>
        <b>ü§ù Join Us!</b> We welcome researchers, developers, and AI enthusiasts to contribute together and advance AI research development. Whether it's code contributions, bug reports, feature suggestions, or documentation improvements, every contribution is valuable!
        <br>üí° <i>Let's build a smarter AI research assistant together!</i>
      </li>
      <li><strong>[2025, Mar 04]</strong>: &nbsp;üéâüéâWe've launched <b>AI-Researcher!</b>, The release includes the complete framework, datasets, benchmark construction pipeline, and much more. Stay tuned‚Äîthere's plenty more to come! üöÄ</li>
    </ul>
</div>

<span id='table-of-contents'/>

## üìë Table of Contents


* <a href='#news'>üî• News</a>
* <a href='#quick-start'>‚ö° Quick Start</a>
  * <a href='#installation'>Installation</a>
  * <a href='#api-keys-setup'>API Keys Setup</a>
* <a href='#examples'>‚¨áÔ∏è Examples</a>
* <a href='#how-it-works'>‚ú® How AI-Researcher works</a>
* <a href='#how-to-use'>üîç How to use AI-Researcher</a>
* <a href='#documentation'>üìñ Documentation</a>
* <a href='#community'>ü§ù Join the Community</a>
* <a href='#acknowledgements'>üôè Acknowledgements</a>
* <a href='#cite'>üåü Cite</a>


<span id='quick-start'/>

## ‚ö° Quick Start

<span id='installation'/>

### Installation

#### AI Installation

1. Using [uv](https://docs.astral.sh/uv/)

> We recommend to use [uv](https://docs.astral.sh/uv/) to manage packages in our project (Much more faster than conda)

```bash
# install uv
curl -LsSf https://astral.sh/uv/install.sh | sh
source ~/.bashrc

# clone the project
git clone https://github.com/HKUDS/AI-Researcher.git
cd AI-Researcher

# install and activate enviroment
uv venv --python 3.11
source ./.venv/bin/activate
uv pip install -e .
playwright install
```

#### Docker Installation

To set up the agent-interactive environment, we use Docker for containerization. Please ensure you have [Docker](https://www.docker.com/) installed on your system before proceeding. For running the research agent, we utilize the Docker image 'tjbtech1/airesearcher:v1t'. You can pull this image by executing the following command:

```bash
docker pull tjbtech1/airesearcher:v1
```

or you can build the docker image from our provided [Dockerfile](./docker/Dockerfile). 

```bash
cd ./docker && docker build -t tjbtech1/airesearcher:v1 .
```

<span id='api-keys-setup'/>

### API Keys Setup

Create an environment variable file based on the provided '.env.template' file. In this file, you should set the configuration including api key, instance id of the test case. 

```bash

# ================ container configuration ================
# workplace of the research agent
DOCKER_WORKPLACE_NAME=workplace_paper
# base image of the research agent
BASE_IMAGES=tjbtech1/airesearcher:v1
# completion model name, configuration details see: https://docs.litellm.ai/docs/
COMPLETION_MODEL=openrouter/google/gemini-2.5-pro-preview-05-20
# cheep model name, configuration details see: https://docs.litellm.ai/docs/
CHEEP_MODEL=openrouter/google/gemini-2.5-pro-preview-05-20
# specific gpu of the research agent, can be: 
# '"device=0"' using the first gpu
# '"device=0,1"' using the first and second gpu
# '"all"' using all gpus
# None for no gpu
GPUS='"device=0"'
# name of the container
CONTAINER_NAME=paper_eval
# name of the workplace
WORKPLACE_NAME=workplace
# path of the cache
CACHE_PATH=cache
# port of the research agent
PORT=7020
# platform of the research agent
PLATFORM=linux/amd64

# ================ llm configuration ================
# github ai token of the research agent
GITHUB_AI_TOKEN=your_github_ai_token
# openrouter api key of the research agent
OPENROUTER_API_KEY=your_openrouter_api_key
# openrouter api base url of the research agent
OPENROUTER_API_BASE=https://openrouter.ai/api/v1

# ================ task configuration ================
# category of the research agent, based on: ./benchmark/final. Can be: 
# diffu_flow
# gnn
# reasoning
# recommendation
# vq
# example: ./benchmark/final/vq
CATEGORY=vq
# instance id of the research agent, example: ./benchmark/final/vq/one_layer_vq.json
INSTANCE_ID=one_layer_vq
# task level of the research agent, can be: 
# task1
# task2
TASK_LEVEL=task1
# maximum iteration times of the research agent
MAX_ITER_TIMES=0
```

### üî• Web GUI

We add a webgui based on gradio. Just run the following command: 

```bash
python web_ai_researcher.py
```

![image-20250606135137558](./assets/webgui/image-20250606135137558.png)

You can configure the environment variables in the following tab: 

![image-20250606135325373](./assets/webgui/image-20250606135325373.png)

Select the following example to run our AI-Researcher: 

<img src="./assets/webgui/image-20250606135507970.png" alt="image-20250606135507970" style="zoom:67%;" />



<span id='examples'/>

## ‚¨áÔ∏è Examples

> ‚ö†Ô∏è **ALERT**: The GIFs below are large files and may **take some time to load**. **Please be patient while they render completely**.

### Example 1 (Vector Quantized)

<details>
  <summary><b>Input:Prompt</b><br><p>I have some reference papers, please implement the following idea with these papers:</p><ol>
    <li>The proposed model designed in this paper is designed to improve the performance of Vector Quantized Variational AutoEncoders (VQ-VAEs) by addressing issues with gradient propagation through the non-differentiable vector quantization layer.</li>...</ol></summary>
<div>
  <!-- <p>I have some reference papers, please implement the following idea with these papers:</p> -->
  <ol start="2">
    <li>The core methodologies utilized include:
      <ul>
        <li><strong>Rotation and Rescaling Transformation</strong>: A linear transformation that alters the encoder output to align it with the nearest codebook vector without changing the forward pass output.</li>
        <li><strong>Gradient Propagation Method</strong>: The proposed model ensures that gradients flow from the decoder to the encoder while preserving the angle between the gradient and codebook vector.</li>
        <li><strong>Codebook Management</strong>: Utilizes the connection between the encoder output and the corresponding codebook vectors to mitigate codebook collapse and improve utilization.</li>
      </ul>
    </li>
    <li>The primary functions of these components are:
      <ul>
        <li>The rotation and rescaling transformation modifies how the encoder output is quantized and how information is retained during backpropagation, enabling gradients to reflect the true positioning of the encoder output relative to the codebook vectors.</li>
        <li>The gradient propagation method redefines how gradients are transported back to the encoder, allowing for an enhanced and nuanced movement through the quantization layer, which leads to a better performance during training.</li>
        <li>Codebook management practices help in maintaining a diverse set of codebook vectors throughout training, avoiding scenarios where multiple vectors become redundant or unused.</li>
      </ul>
    </li>
    <li>Implementation details for each component:
      <ul>
        <li><strong>Key Parameters</strong>: 
          <ul>
            <li>Codebook size should be configured based on the complexity of the dataset (e.g., 1024 or 8192).</li>
            <li>Commitment loss coefficient (Œ≤) is typically set within [0.25, 2].</li>
          </ul>
        </li>
        <li><strong>Input/Output Specifications</strong>: 
          <ul>
            <li>Input to the encoder is a continuous high-dimensional vector, while the output is a corresponding quantized vector from the codebook.</li>
            <li>The output for reconstruction is generated using the decoder applied to the transformed codebook vectors.</li>
          </ul>
        </li>
        <li><strong>Important Constraints</strong>: 
          <ul>
            <li>Ensure that the codebook is updated correctly with an exponential moving average procedure, and treat both rotation and rescaling during the forward pass as constants with respect to the gradient.</li>
          </ul>
        </li>
      </ul>
    </li>
    <li>Step-by-Step Integration of Components:
      <ul>
        <li><strong>Step 1</strong>: Input the data vector into the encoder to obtain the continuous representation.</li>
        <li><strong>Step 2</strong>: Identify the nearest codebook vector to the encoder output.</li>
        <li><strong>Step 3</strong>: Compute the rotation matrix that aligns the encoder output to the codebook vector.</li>
        <li><strong>Step 4</strong>: Apply the rotation and rescaling transformation to obtain the modified output for the decoder (i.e., `Àú q`).</li>
        <li><strong>Step 5</strong>: Feed `Àú q` into the decoder to produce the reconstructed output.</li>
        <li><strong>Step 6</strong>: Compute the loss using the reconstruction and apply backpropagation.</li>
        <li><strong>Step 7</strong>: During backpropagation, modify the gradient transfer process to maintain the angle using the proposed model, replacing traditional shortcuts in gradient computation.</li>
      </ul>
    </li>
    <li>Critical implementation details affecting performance:
      <ul>
        <li>The choice of rotation matrix calculation should ensure computational efficiency‚Äîusing Householder transformations to minimize resource demands.</li>
        <li>The deployment of the stop-gradient technique effectively turns off the back-propagation through the quantization layer, which is essential to reflect the intended change without inducing undesired noise in the gradient updates.</li>
        <li>Monitor the codebook usage regularly during training to detect any potential collapse early and adjust the training dynamics (e.g., learning rate) accordingly to maintain effective utilization throughout the training period.</li>
      </ul>
    </li>
  </ol>
</div>
   </details>

<details>
  <summary><b>Input:Reference Papers</b><ol>
    <li><strong>Neural discrete representation learning</strong></li>...</ol></summary>
<div>
  <ol start="2">
    <!-- <li><strong>Neural discrete representation learning</strong></li> -->
    <li><strong>Straightening out the straight-through estimator: Overcoming optimization challenges in vector quantized networks</strong></li>
    <li><strong>Estimating or propagating gradients through stochastic neurons for conditional computation</strong></li>
    <li><strong>High-resolution image synthesis with latent diffusion models</strong></li>
    <li><strong>Finite scalar quantization: Vq-vae made simple</strong></li>
    <li><strong>Elements of information theory</strong></li>
    <li><strong>Vector-quantized image modeling with improved vqgan</strong></li>
    <li><strong>Uvim: A unified modeling approach for vision with learned guiding codes</strong></li>
    <li><strong>Auto-encoding variational bayes</strong></li>
    <li><strong>Categorical reparameterization with gumbel-softmax</strong></li>
  </ol>
</div>
</details>
<table>
<tr align="center">
    <td width="50%">
        <a href="./examples/rotation_vq/paper.pdf" target="_blank">
        <img src="./examples/rotation_vq/paper.gif" alt="PDF Document" width="100%"/>
    </a>
    <br>
    <em>Self-Organized Paper (fully-generated by AI-Researcher, click to view).</em>
    </td>
    <td width="50%">
        <a href="./examples/rotation_vq/project" target="_blank">
        <img src="./examples/rotation_vq/scrolling_code.gif" alt="profiles" width="100%"/></a>
        <br>
        <em>Self-Organized Workplace, <b>take time to load</b> (fully-generated by AI-Researcher, click to view).</em>
    </td>
</tr>
</table>



### Example 2 (Category: Vector Quantized)

<details>
  <summary><b>Input:Prompt</b><br><p>I have some reference papers, please implement the following idea with these papers:</p><ol>
    <li>The proposed model focuses on discrete representation learning for tasks such as image generation, depth estimation, colorization, and segmentation using the proposed approach integrated into architectures like autoregressive transformers.</li>...</ol></summary>
<div>
  <!-- <p>I have some reference papers, please implement the following idea with these papers:</p> -->
  <ol start="2">
    <li>Core Techniques:
      <ul>
        <li><strong>Simplified Quantization</strong>: Use a simplified quantization approach utilizing scalar quantization instead of VQ.</li>
        <li><strong>Dimensionality Projection</strong>: Define a function to project the encoder output to a manageable dimensionality (typically between 3 to 10).</li>
        <li><strong>Gradient Propagation</strong>: Implement the Straight-Through Estimator (STE) for gradient propagation through the quantization operation.</li>
      </ul>
    </li>
    <li>Technical Components:
      <ul>
        <li><strong>Bounding Function</strong>: This compresses data dimensionality and confines values to a desired range. Use a function like \(f(z) = \left\lfloor \frac{L}{2} \right\rfloor \tanh(z)\) to project the data, where \(L\) is the number of quantization levels.</li>
        <li><strong>Quantization process</strong>: Round each bounded dimension to its nearest integer to yield the quantized output.</li>
        <li><strong>Loss function</strong>: Operate under a reconstruction loss paradigm typical in VAEs to optimize the proposed model parameters.</li>
      </ul>
    </li>
    <li>Implementation Details:
      <ul>
        <li><strong>Key Parameters</strong>: 
          <ul>
            <li>Number of dimensions \(d\) and levels \(L\) per dimension should be defined based on the codebook size you aim to replicate (e.g., set \(L_i \geq 5\) for all \(i\)).</li>
          </ul>
        </li>
        <li><strong>Input/Output Specifications</strong>: 
          <ul>
            <li>The input to the bounding function will be the output from the final encoder layer; the output after quantization will be in the format \(\hat{z}\), with shape matching the original \(z\).</li>
          </ul>
        </li>
        <li><strong>Constraints</strong>: 
          <ul>
            <li>Ensure all inputs are preprocessed adequately to be within the functioning range of the bounding function.</li>
          </ul>
        </li>
      </ul>
    </li>
    <li>Step-by-Step Integration:
      <ul>
        <li><strong>Step 1</strong>: Train a standard VAE model and obtain its encoder output \(z\).</li>
        <li><strong>Step 2</strong>: Apply the bounding function \(f\) on \(z\) to limit the output dimensions to usable values.</li>
        <li><strong>Step 3</strong>: Quantize the resultant bounded \(z\) using the rounding procedure to generate \( \hat{z} \).</li>
        <li><strong>Step 4</strong>: Use the original \(z\) and \(\hat{z}\) in conjunction with the reconstruction loss to backpropagate through the network using the STE for gradient calculation.</li>
      </ul>
    </li>
    <li>Critical Implementation Details:
      <ul>
        <li>Ensure the rounding process is correctly differentiable; utilize the STE to maintain gradient flow during backpropagation.</li>
        <li>Maintain high codebook utilization by selecting optimal dimensions and levels based on empirical trials, and monitor performance to refine the parameters if needed.</li>
        <li>Adjust the proposed model configurations (number of epochs, batch size) based on the structures laid out in this paper, ensuring hyperparameters match those recommended for the proposed approach integration.</li>
      </ul>
    </li>
  </ol>
</div>
   </details>

<details>
  <summary><b>Input:Reference Papers</b><ol>
    <li><strong>Neural discrete representation learning</strong></li>...</ol></summary>
<div>
  <ol start="2">
    <!-- <li><strong>Neural discrete representation learning</strong></li> -->
    <li><strong>Conditional probability models for deep image compression</strong></li>
    <li><strong>High-fidelity generative image compression</strong></li>
    <li><strong>End-to-end optimized image compression</strong></li>
    <li><strong>Taming transformers for high-resolution image generation</strong></li>
    <li><strong>An algorithm for vector quantizer design</strong></li>
    <li><strong>Joint autoregressive and hierarchical priors for learned image compression</strong></li>
    <li><strong>Assessing generative models via precision and recall</strong></li>
    <li><strong>Variational bayes on discrete representation with self-annealed stochastic quantization</strong></li>
    <li><strong>High quality monocular depth estimation via transfer learning</strong></li>
  </ol>
</div>
</details>

<table>
<tr align="center">
    <td width="50%">
        <a href="./examples/fsq/paper.pdf" target="_blank">
        <img src="./examples/fsq/paper.gif" alt="PDF Document" width="100%"/>
    </a>
    <br>
    <em>Self-Organized Paper (fully-generated by AI-Researcher, click to view).</em>
    </td>
    <td width="50%">
        <a href="./examples/fsq/project" target="_blank">
        <img src="./examples/fsq/scrolling_code.gif" alt="profiles" width="100%"/></a>
        <br>
        <em>Self-Organized Workplace, <b>take time to load</b> (fully-generated by AI-Researcher, click to view).</em>
    </td>
</tr>
</table>

### Example 3 (Category: Recommendation)

<details>
  <summary><b>Input:Prompt</b><br><p>I have some reference papers, please implement the following idea with these papers:</p><ol>
    <li>The proposed model aims to improve user-item interaction predictions in recommendation systems by leveraging heterogeneous relational information.</li>...</ol></summary>
<div>
  <!-- <p>I have some reference papers, please implement the following idea with these papers:</p> -->
  <ol start="2">
    <li>Core Techniques/Algorithms:
      <ul>
        <li><strong>Heterogeneous Graph Neural Networks (GNNs)</strong>: Used for embedding initialization and message propagation across different types of user-item and user-user/item-item graphs.</li>
        <li><strong>Contrastive Learning</strong>: Specifically, a cross-view contrastive learning framework is utilized to enhance representation learning by aligning embeddings from auxiliary views with user-item interaction embeddings.</li>
        <li><strong>Meta Networks</strong>: Employed to extract personalized knowledge and facilitate customized knowledge transfer between auxiliary views and the user-item interaction view.</li>
      </ul>
    </li>
    <li>Purpose and Function of Each Major Component:
      <ul>
        <li><strong>Heterogeneous GNN</strong>: Encodes user and item relationships into embeddings that capture the semantics of various interactions.</li>
        <li><strong>Contrastive Learning</strong>: Provides self-supervision signals to enhance the robustness of learned representations, allowing the proposed model to distinguish between relevant and irrelevant interactions.</li>
        <li><strong>Meta Network</strong>: Models personalized characteristics to facilitate adaptive knowledge transfer, ensuring that the influence of auxiliary information is tailored to individual users and items.</li>
      </ul>
    </li>
    <li>Implementation Details:
      <ul>
        <li><strong>Heterogeneous GNN</strong>: 
          <ul>
            <li><strong>Key Parameters</strong>: Use Xavier initializer for embedding initialization; set the hidden dimensionality <code>d</code>.</li>
            <li><strong>Input/Output</strong>: Take adjacency matrices for user-item, user-user, and item-item graphs as input; output relation-aware embeddings.</li>
            <li><strong>Constraints</strong>: Ensure that the GNN can handle varying types of nodes and relations.</li>
          </ul>
        </li>
        <li><strong>Contrastive Learning</strong>: 
          <ul>
            <li><strong>Key Parameters</strong>: Use cosine similarity as the similarity function; define a temperature coefficient for handling negative samples.</li>
            <li><strong>Input/Output</strong>: Input embeddings from the meta network and user/item views; output contrastive loss values.</li>
            <li><strong>Constraints</strong>: Maintain diverse representations to avoid overfitting.</li>
          </ul>
        </li>
        <li><strong>Meta Network</strong>: 
          <ul>
            <li><strong>Key Parameters</strong>: Set up fully connected layers with PReLU activation to generate personalized transformation matrices.</li>
            <li><strong>Input/Output</strong>: Input user and item embeddings; output transformed embeddings for personalized knowledge transfer.</li>
            <li><strong>Constraints</strong>: Ensure low-rank decomposition of transformation matrices to reduce parameter count.</li>
          </ul>
        </li>
      </ul>
    </li>
    <li>Step-by-Step Interaction:
      <ul>
        <li>Initialize user and item embeddings using a heterogeneous GNN.</li>
        <li>Perform heterogeneous message propagation to refine embeddings iteratively across user-item, user-user, and item-item graphs.</li>
        <li>Aggregate the refined embeddings from various views using a mean pooling function to retain heterogeneous semantics.</li>
        <li>Extract meta knowledge from the learned embeddings to create personalized mapping functions using the meta network.</li>
        <li>Apply contrastive learning to align embeddings from auxiliary views with the user-item interaction embeddings, generating a contrastive loss.</li>
        <li>Combine the contrastive loss with a pairwise loss function (like Bayesian Personalized Ranking) to optimize the proposed model.</li>
      </ul>
    </li>
    <li>Critical Implementation Details:
      <ul>
        <li>Choose appropriate hyperparameters such as embedding size, learning rate, and the number of GNN layers through systematic experimentation.</li>
        <li>Monitor the proposed model for signs of overfitting, especially when increasing the number of GNN layers or embedding dimensions.</li>
        <li>Ensure diverse user-item interaction patterns are captured through sufficient training data and effective augmentation techniques.</li>
      </ul>
    </li>
  </ol>
</div>
   </details>

<details>
  <summary><b>Input:Reference Papers</b><ol>
    <li><strong>Revisiting Graph Based Collaborative Filtering: A Linear Residual Graph Convolutional Network Approach</strong></li>...</ol></summary>
<div>
  <ol start="2">
    <!-- <li><strong>Revisiting Graph Based Collaborative Filtering: A Linear Residual Graph Convolutional Network Approach</strong></li> -->
    <li><strong>Graph Neural Networks for Social Recommendation</strong></li>
    <li><strong>Improving Graph Collaborative Filtering with Neighborhood-enriched Contrastive Learning</strong></li>
    <li><strong>LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation</strong></li>
    <li><strong>Knowledge-aware Coupled Graph Neural Network for Social Recommendation</strong></li>
    <li><strong>Heterogeneous Graph Transformer</strong></li>
    <li><strong>Sequential Recommendation with Graph Neural Networks</strong></li>
  </ol>
</div>
</details>

<table>
<tr align="center">
    <td width="50%">
        <a href="./examples/hgcl/paper.pdf" target="_blank">
        <img src="./examples/hgcl/paper.gif" alt="PDF Document" width="100%"/>
    </a>
    <br>
    <em>Self-Organized Paper (fully-generated by AI-Researcher, click to view).</em>
    </td>
    <td width="50%">
        <a href="./examples/hgcl/project" target="_blank">
        <img src="./examples/hgcl/scrolling_code.gif" alt="profiles" width="100%"/></a>
        <br>
        <em>Self-Organized Workplace, <b>take time to load</b> (fully-generated by AI-Researcher, click to view).</em>
    </td>
</tr>
</table>

### Example 4 (Category: Recommendation)

<details>
  <summary><b>Input:Prompt</b><br><p>I have some reference papers, please implement the following idea with these papers:</p><ol>
    <li>The proposed model focuses on collaborative filtering for recommendation systems by leveraging graph neural networks (GNNs) and contrastive learning to address the issue of sparse user-item interactions.</li>...</ol></summary>
<div>
  <!-- <p>I have some reference papers, please implement the following idea with these papers:</p> -->
  <ol start="2">
    <li>Core Techniques:
      <ul>
        <li><strong>Graph Neural Networks</strong>: Utilize GNNs for message passing to learn user and item embeddings from the interaction graph.</li>
        <li><strong>Disentangled Representations</strong>: Implement a mechanism to model multiple latent intent factors driving user-item interactions.</li>
        <li><strong>Contrastive Learning</strong>: Use contrastive learning techniques to generate adaptive self-supervised signals from augmented views of user-item interactions.</li>
      </ul>
    </li>
    <li>Purpose of Components:
      <ul>
        <li><strong>GNN Layers</strong>: Capture high-order interactions among users and items through iterative message passing.</li>
        <li><strong>Intent Encoding</strong>: Differentiate latent intents to improve the representation of user preferences.</li>
        <li><strong>Adaptive Augmentation</strong>: Generate contrastive views that account for both local and global dependencies to enhance robustness against noise.</li>
      </ul>
    </li>
    <li>Implementation Details:
      <ul>
        <li><strong>Graph Construction</strong>: 
          <ul>
            <li><strong>Input</strong>: User-item interaction matrix \( A \) of size \( I \times J \) (where \( I \) is the number of users and \( J \) is the number of items).</li>
            <li><strong>Output</strong>: Normalized adjacency matrix \( \bar{A} \).</li>
          </ul>
        </li>
        <li><strong>GNN Configuration</strong>: 
          <ul>
            <li>Number of layers \( L \): Choose based on your dataset, typically 2 or 3 layers.</li>
            <li>Dimensionality \( d \) of embeddings: Start with \( d = 32 \).</li>
          </ul>
        </li>
        <li><strong>Intent Prototypes</strong>: 
          <ul>
            <li>Number of intents \( K \): Experiment with values from {32, 64, 128, 256}, starting with \( K = 128 \).</li>
          </ul>
        </li>
        <li><strong>Learning Rate</strong>: Use Adam optimizer with a learning rate around \( 1e-3 \).</li>
        <li><strong>Loss Functions</strong>: 
          <ul>
            <li>Use Bayesian Personalized Ranking (BPR) loss for the recommendation task.</li>
            <li>Implement InfoNCE loss for contrastive learning, incorporating both local and global augmented views.</li>
          </ul>
        </li>
      </ul>
    </li>
    <li>Step-by-Step Interaction:
      <ul>
        <li>Construct the interaction graph from the user-item matrix.</li>
        <li>For each GNN layer:
          <ul>
            <li>Compute the aggregated embeddings \( Z(u) \) and \( Z(v) \) using the normalized adjacency matrix.</li>
            <li>Update user and item embeddings using residual connections to prevent over-smoothing.</li>
          </ul>
        </li>
        <li>Generate intent-aware representations by aggregating embeddings over the latent intents.</li>
        <li>Apply the learned parameterized masks for adaptive augmentation during message passing to create multiple contrastive views.</li>
        <li>Calculate contrastive learning signals using the generated augmented representations and optimize using the combined loss function.</li>
      </ul>
    </li>
    <li>Critical Implementation Details:
      <ul>
        <li>Ensure that the augmentation matrices are learned adaptively based on the current user-item embeddings to differentiate the importance of interactions.</li>
        <li>Monitor the performance with different numbers of latent intents \( K \) to find an optimal balance between expressiveness and noise.</li>
        <li>Regularly assess the proposed model for over-smoothing by checking the Mean Average Distance (MAD) metric on the embeddings.</li>
        <li>Tune hyperparameters \( \lambda_1, \lambda_2, \lambda_3 \) for the multi-task loss to balance the contribution of the self-supervised learning signals.</li>
      </ul>
    </li>
  </ol>
</div>
   </details>

<details>
  <summary><b>Input:Reference Papers</b><ol>
    <li><strong>Lightgcn: Simplifying and powering graph convolution network for recommendation</strong></li>...</ol></summary>
<div>
  <ol start="2">
    <!-- <li><strong>Lightgcn: Simplifying and powering graph convolution network for recommendation</strong></li> -->
    <li><strong>Neural collaborative filtering</strong></li>
    <li><strong>Disentangled contrastive learning on graphs</strong></li>
    <li><strong>Improving Graph Collaborative Filtering with Neighborhood-enriched Contrastive Learning</strong></li>
    <li><strong>Curriculum Disentangled Recommendation with Noisy Multi-feedback</strong></li>
    <li><strong>Disentangled heterogeneous graph attention network for recommendation</strong></li>
    <li><strong>Learning intents behind interactions with knowledge graph for recommendation</strong></li>
    <li><strong>LightGCL: Simple Yet Effective Graph Contrastive Learning for Recommendation</strong></li>
    <li><strong>Self-supervised graph learning for recommendation</strong></li>
  </ol>
</div>
</details>

<table>
<tr align="center">
    <td width="50%">
        <a href="./examples/dccf/paper.pdf" target="_blank">
        <img src="./examples/dccf/paper.gif" alt="PDF Document" width="100%"/>
    </a>
    <br>
    <em>Self-Organized Paper (fully-generated by AI-Researcher, click to view).</em>
    </td>
    <td width="50%">
        <a href="./examples/dccf/project" target="_blank">
        <img src="./examples/dccf/scrolling_code.gif" alt="profiles" width="100%"/></a>
        <br>
        <em>Self-Organized Workplace, <b>take time to load</b> (fully-generated by AI-Researcher, click to view).</em>
    </td>
</tr>
</table>

### Example 5 (Category: Diffusion and Flow Matching)

<details>
  <summary><b>Input:Prompt</b><br><p>I have some reference papers, please implement the following idea with these papers:</p><ol>
    <li>The proposed model presented in this paper focuses on the task of generative modeling through the framework of Continuous Normalizing Flows (CNFs) to define straight flows between noise and data samples.</li>...</ol></summary>
<div>
  <!-- <p>I have some reference papers, please implement the following idea with these papers:</p> -->
  <ol start="2">
    <li>Architecture:
      <ul>
        <li>Implement a neural network to parameterize the velocity field \( v_{\theta}(t, x) \) that maps from noise to data distributions.</li>
        <li>Use architectures suitable for continuous functions, such as feedforward or convolutional networks.</li>
        <li>Each layer should have non-linear activation functions (e.g., ReLU, Tanh).</li>
      </ul>
    </li>
    <li>Loss Functions:
      <ul>
        <li><strong>Velocity Consistency Loss</strong>: This should be structured as:
          \[
          L_{\theta} = E_{t \sim U} E_{x_t, x_{t+\Delta t}} \| f_{\theta}(t, x_t) - f_{\theta}(t+\Delta t, x_{t+\Delta t}) \|^2_2 + \alpha \| v_{\theta}(t, x_t) - v_{\theta}(t+\Delta t, x_{t+\Delta t}) \|^2_2
          \]
          where \( f_{\theta}(t, x_t) = x_t + (1 - t) v_{\theta}(t, x_t) \). Choose \( \alpha \) based on cross-validation performance.
        </li>
      </ul>
    </li>
    <li>Training Procedure:
      <ul>
        <li>Sample \( x_0 \) from the noise distribution \( p_0 \).</li>
        <li>For multiple time segments, define intervals and compute velocity fields iteratively.</li>
        <li>Use the weights of the proposed approach in an exponential moving average to stabilize training.</li>
      </ul>
    </li>
    <li>Sampling Process:
      <ul>
        <li>For single-step or multi-step generation, heuristically sample from the noise distribution and use the learned velocity field as follows: 
          \[
          x_{i/k} = x_{(i-1)/k} + \frac{1}{k} v_{i\theta}((i-1)/k, x_{(i-1)/k})
          \]
        </li>
        <li>Apply the Euler method for iterative updates:
          \[
          x_{t + \Delta t} = x_t + \Delta t v_i(t, x_t)
          \]
          where \( t \in [i/k, (i + 1)/k - \Delta t] \).
        </li>
      </ul>
    </li>
    <li>Key Implementation Details:
      <ul>
        <li>Ensure the network is equipped with a suitable optimizer such as Adam with a learning rate around \( 2 \times 10^{-4} \).</li>
        <li>The batch size should be appropriately set (e.g., 512 for CIFAR-10).</li>
        <li>Employ an ODE solver, suggested as Euler's method, during the training and sampling processes.</li>
        <li>Maintain a uniform distribution for sampling time intervals \( U \).</li>
      </ul>
    </li>
    <li>Performance Considerations:
      <ul>
        <li>Monitor convergence rates and empirically validate parameter configurations through experiments. Start with fewer segments and gradually increase to capture complex distributions better.</li>
        <li>Adjust the decay rate for the EMA based on the stability of convergence (commonly around 0.999).</li>
        <li>Analyze the trade-offs between sampling efficiency and sample quality, ensuring a balance during proposed model development.</li>
      </ul>
    </li>
  </ol>
</div>
   </details>

<details>
  <summary><b>Input:Reference Papers</b><ol>
    <li><strong>Flow matching for generative modeling</strong></li>...</ol></summary>
<div>
  <ol start="2">
    <!-- <li><strong>Flow matching for generative modeling</strong></li> -->
    <li><strong>Consistency models</strong></li>
    <li><strong>Rectified Flow</strong></li>
    <li><strong>Denoising diffusion probabilistic models</strong></li>
    <li><strong>Optimal flow matching: Learning straight trajectories in just one step</strong></li>
    <li><strong>Maximum likelihood training of score-based diffusion models</strong></li>
    <li><strong>Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow</strong></li>
  </ol>
</div>
</details>

<table>
<tr align="center">
    <td width="50%">
        <a href="./examples/con_flowmatching/paper.pdf" target="_blank">
        <img src="./examples/con_flowmatching/paper.gif" alt="PDF Document" width="100%"/>
    </a>
    <br>
    <em>Self-Organized Paper (fully-generated by AI-Researcher, click to view).</em>
    </td>
    <td width="50%">
        <a href="./examples/con_flowmatching/project" target="_blank">
        <img src="./examples/con_flowmatching/scrolling_code.gif" alt="profiles" width="100%"/></a>
        <br>
        <em>Self-Organized Workplace, <b>take time to load</b> (fully-generated by AI-Researcher, click to view).</em>
    </td>
</tr>
</table>

### Example 6 (Category: Graph Neural Networks)

<details>
  <summary><b>Input:Prompt</b><br><p>I have some reference papers, please implement the following idea with these papers:</p><ol>
    <li>The proposed model focuses on the task of node classification in large graphs, addressing challenges like scalability, heterophily, long-range dependencies, and the absence of edges.</li>...</ol></summary>
<div>
  <!-- <p>I have some reference papers, please implement the following idea with these papers:</p> -->
  <ol start="2">
    <li>The core techniques used in this study include a kernelized Gumbel-Softmax operator for all-pair message passing, which reduces computational complexity to linear (O(N)), and a Transformer-style network architecture designed for layer-wise learning of latent graph structures.</li>
    <li>The purpose of the kernelized Gumbel-Softmax operator is to enable differentiable learning of discrete graph structures by approximating categorical distributions. The Transformer-style architecture facilitates information propagation between arbitrary pairs of nodes through learned latent graphs.</li>
    <li>Implementation details for each component:
      <ul>
        <li><strong>Kernelized Gumbel-Softmax Operator</strong>: Set the temperature parameter (œÑ) to a range typically between 0.25 and 0.4 for training. It operates on node feature representations (D-dimensional feature vectors). The output of this operator is a distribution over node connections, facilitating the selection of neighbors for message passing.</li>
        <li><strong>Node Feature Input</strong>: Each node input should be represented as a feature vector (e.g., {x_u} ‚àà R^D), and the output is an updated representation of the node embedding after message passing.</li>
        <li><strong>Relational Bias (if applicable)</strong>: Introduces activation (e.g., sigmoid) to adjust the message passing weights based on an observed adjacency matrix, which enhances weight assignment for connected nodes.</li>
        <li><strong>Edge Regularization Loss</strong>: Combines categorical edge probabilities with a supervised classification loss, encouraging the network to maintain predicted edges consistent with observed edges.</li>
      </ul>
    </li>
    <li>The step-by-step interaction of these components includes:
      <ul>
        <li>Begin with an input matrix of node embeddings (X) and, if available, an adjacency matrix (A).</li>
        <li>Apply the kernelized Gumbel-Softmax operator to the embedding matrix to generate a probability distribution over neighbor selection for each node.</li>
        <li>Use these probabilities to sample neighbors, allowing for message passing where each node aggregates information from its selected neighbors.</li>
        <li>Update the node embeddings using an attention mechanism, which can be enhanced by relational bias if edges are available.</li>
        <li>After K iterations of neighbor sampling, apply loss functions comprising a supervised classification loss and, if applicable, edge-level regularization loss to optimize the embedding representations.</li>
      </ul>
    </li>
    <li>Critical implementation details affecting performance involve:
      <ul>
        <li>Careful tuning of the temperature parameter (œÑ) in the Gumbel-Softmax operator, as it significantly influences the proposed approach's capacity to capture the discrete nature of graph structures.</li>
        <li>Utilizing appropriate batch sizes for large-scale graphs, ensuring enough memory is available while also maintaining computational efficiency.</li>
        <li>Choosing the correct dimensionality for random features in the kernel approximation, balancing model expressiveness and training stability.</li>
        <li>The use of dropout or other regularization techniques such as edge-level regularization can influence the proposed model's generalization capabilities on unseen data.</li>
      </ul>
    </li>
  </ol>
</div>
   </details>

<details>
  <summary><b>Input:Reference Papers</b><ol>
    <li><strong>On the bottleneck of graph neural networks and its practical implications</strong></li>...</ol></summary>
<div>
  <ol start="2">
    <!-- <li><strong>On the bottleneck of graph neural networks and its practical implications</strong></li> -->
    <li><strong>Semi-supervised classification with graph convolutional networks</strong></li>
    <li><strong>Categorical reparameterization with gumbel-softmax</strong></li>
    <li><strong>Learning discrete structures for graph neural networks</strong></li>
    <li><strong>Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</strong></li>
    <li><strong>Graph attention networks</strong></li>
    <li><strong>Geometric deep learning: going beyond euclidean data</strong></li>
    <li><strong>Graph structure learning for robust graph neural networks</strong></li>
    <li><strong>Geom-gcn: Geometric graph convolutional networks</strong></li>
    <li><strong>New benchmarks for learning on non-homophilous graphs</strong></li>
    <li><strong>Latent patient network learning for automatic diagnosis</strong></li>
    <li><strong>Few-shot learning with graph neural networks</strong></li>
    <li><strong>The graph neural network model</strong></li>
    <li><strong>Characteristic functions on graphs: Birds of a feather, from statistical descriptors to parametric models</strong></li>
    <li><strong>Beyond homophily in graph neural networks: Current limitations and effective designs</strong></li>
  </ol>
</div>
</details>

<table>
<tr align="center">
    <td width="50%">
        <a href="./examples/gnn_nodeformer/paper.pdf" target="_blank">
        <img src="./examples/gnn_nodeformer/paper.gif" alt="PDF Document" width="100%"/>
    </a>
    <br>
    <em>Self-Organized Paper (fully-generated by AI-Researcher, click to view).</em>
    </td>
    <td width="50%">
        <a href="./examples/gnn_nodeformer/project" target="_blank">
        <img src="./examples/gnn_nodeformer/scrolling_code.gif" alt="profiles" width="100%"/></a>
        <br>
        <em>Self-Organized Workplace, <b>take time to load</b> (fully-generated by AI-Researcher, click to view).</em>
    </td>
</tr>
</table>

### Example 7 (Category: Graph Neural Networks)

<details>
  <summary><b>Input:Prompt</b><br><p>I have some reference papers, please implement the following idea with these papers:</p><ol>
    <li>The proposed approach works on the task of uncovering data dependencies and learning instance representations from datasets that may not have complete or reliable relationships, particularly in semi-supervised contexts like node classification, image/text classification, and spatial-temporal dynamics prediction.</li>...</ol></summary>
<div>
  <!-- <p>I have some reference papers, please implement the following idea with these papers:</p> -->
  <ol start="2">
    <li>The core techniques/algorithms used in this paper include an energy-constrained diffusion model represented as a partial differential equation (PDE), an explicit Euler scheme for numerical solutions, and a form of adaptive diffusivity function based on the energy function. The proposed architecture utilizes a diffusion-based Transformer framework that allows for all-pair feature propagation among instances.</li>
    <li>The major technical components serve the following purposes:
      <ul>
        <li><strong>Diffusion Process:</strong> Encodes instances into evolving states by modeling information flow, where instance representations evolve according to a PDE illuminating the relationships among the instances.</li>
        <li><strong>Energy Function:</strong> Provides constraints to regularize the diffusion process and guide the proposed model towards desired low-energy embeddings, enhancing the quality of representations.</li>
        <li><strong>Diffusivity Function:</strong> Specifies the strength of information flow between instances, adapting based on the instance states, and allows for flexible and efficient propagation strategies.</li>
      </ul>
    </li>
    <li>Implementation details for each component:
      <ul>
        <li><strong>Diffusion Process Input:</strong> Requires a batch of instances represented as a matrix of size \(N \times D\), where \(N\) is the number of instances and \(D\) is the input feature dimension.</li>
        <li><strong>Diffusion Process Output:</strong> Produces the updated instance representations after \(K\) propagation steps. The step size \(\tau\) should be set within the range (0, 1).</li>
        <li><strong>Energy Function:</strong> Implemented as \(E(Z, k; \delta) = ||Z - Z^{(k)}||^2_F + \lambda \sum_{i,j} \delta(||z_i - z_j||^2_2)\), with \(\delta\) being a non-decreasing, concave function.</li>
        <li><strong>Key Parameters:</strong>
          <ul>
            <li>Step size \(\tau\)</li>
            <li>Layer number \(K\) (number of diffusion propagation steps)</li>
            <li>Regularization weight \(\lambda\).</li>
          </ul>
        </li>
      </ul>
    </li>
    <li>Step-by-step description of interactions:
      <ul>
        <li>Start by initializing the instance representations.</li>
        <li>For each layer of diffusion, compute the diffusivity \(S(k)\) based on current embeddings through a function \(f\) which can be defined differently depending on the proposed model implementation.</li>
        <li>Update the instance representations using the defined diffusion equations, ensuring to conserve states and introduce propagation according to the computed diffusivity.</li>
        <li>After \(K\) layers of diffusion, apply a final output layer to produce logits for predictions.</li>
      </ul>
    </li>
    <li>Critical implementation details that affect performance:
      <ul>
        <li>The choice of diffusivity function \(f\) greatly impacts the proposed model's capacity to learn complex dependencies, where specific formulations (like linear or logistic) yield different abilities in capturing inter-instance relationships.</li>
        <li>Ensure that the values of \(\tau\) and \(\lambda\) are set appropriately to balance convergence speed and representation quality; using a smaller \(\tau\) may require deeper layers to learn effectively.</li>
        <li>Optimization parameters like learning rate and early stopping criteria are essential, particularly for large-scale datasets where convergence behavior can vary widely depending on architecture size and complexity.</li>
      </ul>
    </li>
  </ol>
</div>
   </details>

<details>
  <summary><b>Input:Reference Papers</b><ol>
    <li><strong>Diffusion-convolutional neural networks</strong></li>...</ol></summary>
<div>
  <ol start="2">
    <!-- <li><strong>Diffusion-convolutional neural networks</strong></li> -->
    <li><strong>Semi-supervised classification with graph convolutional networks</strong></li>
    <li><strong>Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</strong></li>
    <li><strong>Geometric deep learning: going beyond euclidean data</strong></li>
    <li><strong>Artificial neural networks for solving ordinary and partial differential equations</strong></li>
    <li><strong>Scaling graph neural networks with approximate pagerank</strong></li>
    <li><strong>Learning discrete structures for graph neural networks</strong></li>
    <li><strong>Semi-supervised learning using gaussian fields and harmonic functions</strong></li>
    <li><strong>Graph convolutional networks</strong></li>
    <li><strong>Deep learning via semi-supervised embedding</strong></li>
    <li><strong>A generalization of transformer networks to graphs</strong></li>
    <li><strong>Graph Convolution and Quadratic Time Complexity</strong></li>
    <li><strong>Bayesian graph convolutional neural networks for semi-supervised classification</strong></li>
    <li><strong>Do transformers really perform bad for graph representation?</strong></li>
    <li><strong>Big bird: Transformers for longer sequences</strong></li>
    <li><strong>Adaptive graph diffusion networks</strong></li>
    <li><strong>Transformers are RNNs</strong></li>
    <li><strong>Collective classification in network data</strong></li>
    <li><strong>NodeFormer: A scalable graph structure learning transformer for node classification</strong></li>
  </ol>
</div>
</details>

<table>
<tr align="center">
    <td width="50%">
        <a href="./examples/gnn_difformer/paper.pdf" target="_blank">
        <img src="./examples/gnn_difformer/paper.gif" alt="PDF Document" width="100%"/>
    </a>
    <br>
    <em>Self-Organized Paper (fully-generated by AI-Researcher, click to view).</em>
    </td>
    <td width="50%">
        <a href="./examples/gnn_difformer/project" target="_blank">
        <img src="./examples/gnn_difformer/scrolling_code.gif" alt="profiles" width="100%"/></a>
        <br>
        <em>Self-Organized Workplace, <b>take time to load</b> (fully-generated by AI-Researcher, click to view).</em>
    </td>
</tr>
</table>


<span id='how-it-works'/>

## ‚ú®How AI-Researcher works

* üîÑ **End-to-End Scientific Research Automation System**
  <br>Our **AI-Researcher** provides comprehensive automation for the complete scientific research lifecycle through an integrated pipeline. The system orchestrates research activities across three strategic phases:
  1. **Literature Review & Idea Generation** üìöüí°
     * üîç **Resource Collector**: Systematically gathers comprehensive research materials across multiple scientific domains through automated collection from major academic databases (e.g., arXiv, IEEE Xplore, ACM Digital Library, and Google Scholar), code platforms (e.g., GitHub, Hugging Face), and open datasets across scientific domains.
       
     * üß† **Resource Filter**: Evaluates and selects high-impact papers, well-maintained code implementations, and benchmark datasets through quality metrics (e.g., citation count, code maintenance, data completeness) and relevance assessment.
     
     * üí≠ **Idea Generator**: Leveraging the identified research resources, including high-impact papers and code repositories, the Idea Generator systematically formulates novel research directions through comprehensive analysis. It automatedly evaluates current methodological limitations, map emerging technological trends, and explore uncharted research territories.
  
  2. **New Algorithm Design, Implementation & Validation** üß™üíª
     <br>**Design ‚Üí Implementation ‚Üí Validation ‚Üí Refinement**
     * üìù**Design Phase**: The initial phase focuses on conceptual development, where novel algorithmic ideas are formulated and theoretical foundations are established. During this stage, we carefully plan the implementation strategy, ensuring the proposed solution advances beyond existing approaches while maintaining practical feasibility.
     
     * ‚öôÔ∏è**Implementation Phase**: proceed to transform abstract concepts into concrete code implementations. This phase involves developing functional modules, establishing a robust testing environment, and creating necessary infrastructure for experimental validation.
  
     * üî¨**Validation Phase**: Systematic experimentation forms the core of our validation process. We execute comprehensive tests to evaluate algorithm performance, collect metrics, and document all findings. This phase ensures rigorous implementation verification with practical requirements.
       
     * üîß**Refinement Phase** üî¨: Based on validation results, we enter an iterative refinement cycle. This phase involves identifying areas for improvement, optimizing code efficiency, and implementing necessary enhancements. We carefully analyze performance bottlenecks and plan strategic improvements for the next development iteration.
  
  3. **Paper Writing** ‚úçÔ∏èüìù
     * **Writer Agent** üìÑ: Automatically generates full-length academic papers by integrating research ideas, motivations, newly designed algorithm frameworks, and algorithm validation performance. Leveraging a hierarchical writing approach, it creates polished manuscripts with precision and clarity.

üöÄ This fully automated system removes the need for manual intervention across the entire research lifecycle, enabling effortless and seamless scientific discovery‚Äîfrom initial concept to final publication. üöÄ It serves as an excellent research assistant, aiding researchers in achieving their goals efficiently and effectively.

--------------------------------------------------------------------------------

* üî¨ **Comprehensive Benchmark Suite**
  <br>We have developed a comprehensive and standardized evaluation framework to objectively assess the academic capabilities of AI researchers and the quality of their scholarly work, integrating several key innovations to ensure thorough and reliable evaluation.

  1. üë®‚Äçüî¨ **Expert-Level Ground Truth**: TThe benchmark leverages human expert-written papers as ground truth references, establishing a high-quality standard for comparison and validation.

  2. üåà **Multi-Domain Coverage**: Our benchmark is designed to comprehensively span 4 major research domains, ensuring broad applicability: Computer Vision (CV), Nature Language Processing (NLP), Data Mining (DM), and Information Retrieval (IR).

  3. üåê **Fully Open-Source Benchmark Construction**: We have fully open-sourced the methodology and process for building the benchmark, including complete access to processed datasets, data collection pipelines, and processing code. This ensures **Transparency in Evaluation** while empowering the community to customize and construct benchmarks tailored to their specific domains for testing AI researchers.
    
  4. üìä **Comprehensive Evaluation Metrics**: Our evaluation framework adopts a hierarchical and systematic approach, where tasks are organized into two levels based on the extent of idea provision. Leveraging specialized **Evaluator Agents**, the framework conducts thorough assessments across multiple dimensions, ensuring a robust and comprehensive evaluation. Key evaluation metrics include: 1) **Novelty**: Assessing the innovation and uniqueness of the research work. 2) **Experimental Comprehensiveness**: Evaluating the design, execution, and rigor of the experiments. 3) **Theoretical Foundation**: Measuring the strength of the theoretical background and foundations. 4) **Result Analysis**: Analyzing the depth and accuracy of result interpretation. 5) **Writing Quality**: Reviewing the clarity, coherence, and structure of the written report.

üöÄ **Advancing Research Automation**. This benchmark suite provides an objective framework for assessing research automation capabilities. It is designed to evolve continuously, incorporating new advancements and expanding its scope to meet the growing demands of the research community.

--------------------------------------------------------------------------------

* üåü **Easy-to-Use AI Research Assistant**
  <br>**AI-Researcher**E delivers a truly seamless and accessible experience for research automation, empowering users to focus on innovation without technical barriers. Key features include:
  
  1. üåê **Multi-LLM Provider Support**: Effortlessly integrates with leading language model providers such as Claude, OpenAI, Deepseek, and more. Researchers can select the most suitable AI capabilities for their specific needs.
  
  2. üìö **Effortless Research Kickoff**: Kickstart your research journey with unparalleled ease! Simply provide a list of relevant papers, and **AI-Researcher** takes care of the rest‚Äîno need to upload files, contribute initial ideas, or navigate complex configurations. It's the ultimate tool to help you jumpstart your research process efficiently and effectively.
  
  3. üß† **Minimal Domain Expertise Needed**: AI-Researcher simplifies the research process by autonomously identifying critical research gaps, proposing innovative approaches, and executing the entire research pipeline. While some domain understanding can enhance results, the tool is designed to empower users of all expertise levels to achieve impactful outcomes with ease.
  
  4. üì¶ **Out-of-the-Box Functionality**: Experience seamless research automation right from the start. AI-Researcher is ready to use with minimal setup, giving you instant access to advanced capabilities. Skip the hassle of complex configurations and dive straight into accelerating your research process with ease and efficiency.

<span id='how-to-use'/>

## üîç How to use AI-Researcher

### 1. Research Agent

If you want to use research agent with the given idea (Level 1 tasks), conducting extensive survey and experiments,  you can use the following command in the [`research_agent/run_infer_level_1.sh`](./research_agent/run_infer_level_1.sh):

```bash
current_dir=$(dirname "$(readlink -f "$0")")
cd $current_dir
export DOCKER_WORKPLACE_NAME=workplace_paper

export BASE_IMAGES=tjbtech1/paperagent:latest

export COMPLETION_MODEL=claude-3-5-sonnet-20241022
export CHEEP_MODEL=claude-3-5-haiku-20241022

category=vq
instance_id=one_layer_vq
export GPUS='"device=0,1"'

python run_infer_plan.py --instance_path ../benchmark/final/${category}/${instance_id}.json --container_name paper_eval --task_level task1 --model $COMPLETION_MODEL --workplace_name workplace --cache_path cache --port 12372 --max_iter_times 0 --category ${category}
```

If you want to just give the reference papers, and let the research agent to generate the idea then conduct the experiments (Level 2 tasks), you can use the following command in the [`research_agent/run_infer_level_2.sh`](./research_agent/run_infer_level_2.sh):

```bash
current_dir=$(dirname "$(readlink -f "$0")")
cd $current_dir
export DOCKER_WORKPLACE_NAME=workplace_paper

export BASE_IMAGES=tjbtech1/paperagent:latest

export COMPLETION_MODEL=claude-3-5-sonnet-20241022
export CHEEP_MODEL=claude-3-5-haiku-20241022

category=vq
instance_id=one_layer_vq
export GPUS='"device=0,1"'

python run_infer_idea.py --instance_path ../benchmark/final/${category}/${instance_id}.json --container_name paper_eval --model $COMPLETION_MODEL --workplace_name workplace --cache_path cache --port 12372 --max_iter_times 0 --category ${category}
```

### 2. Paper Writing Agent

If you want to generate the paper after the research agent has conducted the research, you can use the following command in the [`paper_agent/run_infer.sh`](./paper_agent/run_paper.sh):

```bash
#!/bin/bash

cd path/to/AI-Researcher/paper_agent

export OPENAI_API_KEY=sk-SKlupNntta4WPmvDCRo7uuPbYGwOnUQcb25Twn8c718tPpXN


research_field=vq
instance_id=rotated_vq

python path/to/AI-Researcher/paper_agent/writing.py --research_field ${research_field} --instance_id ${instance_id}
```

### 3. Benchmark Data and Collection

Our benchmark is also fully-open-sourced: 

* Detailed benchmark data is available in the [`benchmark`](./benchmark) folder.
* Detailed benchmark collection process is available in the [`benchmark_collection`](./benchmark_collection) folder.

<span id='documentation'/>

## üìñ Documentation

Comprehensive documentation is on its way üöÄ! Stay tuned for updates on our [Documentation](https://auto-researcher.github.io/docs) page.

<span id='community'/>

## ü§ù Join the Community

We aim to build a vibrant community around AI-Researcher and warmly invite everyone to join us. Here's how you can become part of our community:

- [Join our Slack workspace](https://join.slack.com/t/ai-researchergroup/shared_invite/zt-30y5a070k-C0ajQt1zmVczFnfGkIicvA) - Here we talk about research, architecture, and future development.
- [Join our Discord server](https://discord.gg/ghSnKGkq) - This is a community-run server for general discussion, questions, and feedback. 
- [Read or post Github Issues](https://github.com/HKUDS/AI-Researcher/issues) - Check out the issues we're working on, or add your own ideas.


## Misc

<div align="center">

[![Stargazers repo roster for @HKUDS/AI-Researcher](https://reporoster.com/stars/HKUDS/AI-Researcher)](https://github.com/HKUDS/AI-Researcher/stargazers)

[![Forkers repo roster for @HKUDS/AI-Researcher](https://reporoster.com/forks/HKUDS/AI-Researcher)](https://github.com/HKUDS/AI-Researcher/network/members)

[![Star History Chart](https://api.star-history.com/svg?repos=HKUDS/AI-Researcher&type=Date)](https://star-history.com/#HKUDS/AI-Researcher&Date)

</div>


<span id='cite'/>

## üåü Cite

A more detailed technical report will be released soon. üöÄ:

```tex
@misc{airesearcher,
      title={{AI-Researcher: Autonomous Scientific Innovation}},
      author={Jiabin Tang, Lianghao Xia, Zhonghang Li, Chao Huang},
      year={2025},
      eprint={2505.18705},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2505.18705},
}
```
